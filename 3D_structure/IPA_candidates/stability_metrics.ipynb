{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9bbf3b-e2b8-4cb0-86a2-74421bb377df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO, pairwise2\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "from Bio.PDB import PDBParser, Superimposer\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53043a69-c6fc-4357-a3cb-2e10e828bf90",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedfb056-c3c0-42f6-a397-251104cac815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RMSD ####\n",
    "\n",
    "    \n",
    "# to calculate rmsd using Bio.PDB\n",
    "# predicted_pdb: moving pdb comparing to reference, need to have the same number of CA atoms as the ref_pdb\n",
    "def rmsd_pdb(predicted_pdb, ref_pdb):\n",
    "    parser = PDBParser()\n",
    "    struct_ref = parser.get_structure(os.path.basename(ref_pdb), ref_pdb)\n",
    "    struct_predicted = parser.get_structure(os.path.basename(predicted_pdb), predicted_pdb)\n",
    "    fixed = [atom for atom in struct_ref[0].get_atoms() if atom.name == \"CA\"]\n",
    "    moving = [atom for atom in struct_predicted[0].get_atoms() if atom.name == \"CA\"]\n",
    "    sup = Superimposer()\n",
    "    # sets the fixed and moving atom lists\n",
    "    # finds the rotation and translation matrix that best superimposes the moving atoms onto fixed atoms\n",
    "    sup.set_atoms(fixed, moving)\n",
    "    # applies the calculated rotation and translation to all atoms in the second structure\n",
    "    # superimposing it onto the first structure\n",
    "    #sup.apply(struct_predicted[0].get_atoms())\n",
    "    sup.apply(moving)\n",
    "\n",
    "    return sup.rms\n",
    "\n",
    "\n",
    "def rmsd_point(coordinate1, coordinate2):\n",
    "    # 3D coordinates, example: array([ 11.925492,  10.070204, -12.518902], dtype=float32)\n",
    "    # this is a function to calculate rmsd for a single point\n",
    "    x1 = coordinate1[0]\n",
    "    y1 = coordinate1[1]\n",
    "    z1 = coordinate1[2]\n",
    "    x2 = coordinate2[0]\n",
    "    y2 = coordinate2[1]\n",
    "    z2 = coordinate2[2]\n",
    "    value = np.sqrt((x1-x2)**2 + (y1-y2)**2 + (z1-z2)**2)\n",
    "    return value\n",
    "\n",
    "\n",
    "def rmsd_list(coordinates1, coordinates2):\n",
    "    # list of 3D coordinates, should have the same number of coordinates\n",
    "    # this is a function to calculate rmsd for two list of 3D coordinates\n",
    "    length = len(coordinates1)\n",
    "    values = []\n",
    "    for i in range(length):\n",
    "        x1 = coordinates1[i][0]\n",
    "        y1 = coordinates1[i][1]\n",
    "        z1 = coordinates1[i][2]\n",
    "        x2 = coordinates2[i][0]\n",
    "        y2 = coordinates2[i][1]\n",
    "        z2 = coordinates2[i][2]\n",
    "        value = (x1-x2)**2 + (y1-y2)**2 + (z1-z2)**2\n",
    "        values.append(value)\n",
    "    rmsd = np.sqrt(sum(values)/length)\n",
    "    return rmsd\n",
    "\n",
    "\n",
    "\n",
    "#### Pairwise Sequence Alignment ####\n",
    "    \n",
    "\n",
    "# load fasta file to get the sequence\n",
    "def load_fasta(fasta_file):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        record = next(SeqIO.parse(f, 'fasta'))\n",
    "        return record.seq\n",
    "\n",
    "\n",
    "# extract common residues from two sequences\n",
    "def common_residues(seq1, seq2):\n",
    "    # inputs are aligned sequences, including gaps, length should be the same\n",
    "    # outputs are the common residues of the two sequences, indices are the original ones (without gaps)\n",
    "    common_indices1 = []\n",
    "    common_indices2 = []\n",
    "    res_idx1 = 0\n",
    "    res_idx2 = 0\n",
    "    for a, b in zip(seq1, seq2):\n",
    "        if a == b:\n",
    "            res_idx1 += 1\n",
    "            res_idx2 += 1\n",
    "            common_indices1.append(res_idx1)\n",
    "            common_indices2.append(res_idx2)\n",
    "        elif (a != b) and (a != '-') and (b != '-'):\n",
    "            res_idx1 += 1\n",
    "            res_idx2 += 1\n",
    "        elif (a == '-') and (b != '-'):\n",
    "            res_idx2 += 1\n",
    "        elif (a != '-') and (b == '-'):\n",
    "            res_idx1 += 1\n",
    "        else:\n",
    "            print(a, b)\n",
    "    if len(common_indices1) != len(common_indices2):\n",
    "        print(\"Two indices have different length!\")\n",
    "        return 1\n",
    "    return common_indices1, common_indices2\n",
    "\n",
    "    \n",
    "# return common indices for each sequence\n",
    "def pairwise_SA(moving_fasta, ref_fasta):\n",
    "    # load sequences from fasta files\n",
    "    seq_moving = load_fasta(moving_fasta)\n",
    "    seq_ref = load_fasta(ref_fasta)\n",
    "\n",
    "    # pairwise sequence alignment\n",
    "    alignments = pairwise2.align.globalxx(seq_moving, seq_ref)\n",
    "    aligned_seq_moving, aligned_seq_ref = alignments[0][:2]\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = common_residues(aligned_seq_moving, aligned_seq_ref)\n",
    "\n",
    "    return indices_moving, indices_ref\n",
    "\n",
    "\n",
    "#### read residue types from pairs ####\n",
    "def residue_type(moving_fasta, ref_fasta):\n",
    "    # load sequences from fasta files\n",
    "    seq_moving = load_fasta(moving_fasta)\n",
    "    seq_ref = load_fasta(ref_fasta)\n",
    "\n",
    "    # pairwise sequence alignment\n",
    "    alignments = pairwise2.align.globalxx(seq_moving, seq_ref)\n",
    "    aligned_seq_moving, aligned_seq_ref = alignments[0][:2]\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = common_residues(aligned_seq_moving, aligned_seq_ref)\n",
    "\n",
    "    # initialize dictionary to save residue types\n",
    "    residue_moving = {}\n",
    "    residue_ref = {}\n",
    "\n",
    "    # save the residue\n",
    "    for i in range(len(indices_ref)):\n",
    "        idx_moving = indices_moving[i] \n",
    "        idx_ref = indices_ref[i]\n",
    "        residue_moving[idx_ref] = seq_moving[idx_moving-1] # use reference index\n",
    "        residue_ref[idx_ref] = seq_ref[idx_ref-1] # change 1-based index to 0-based index\n",
    "\n",
    "    # check if the two dictionaries are the same\n",
    "    if residue_moving != residue_ref:\n",
    "        print(\"residue inconsistency!\")\n",
    "        return 1\n",
    "\n",
    "    # only need to return one dictionary\n",
    "    return residue_ref\n",
    "    \n",
    "    \n",
    "#### RMSD with pair ####\n",
    "    \n",
    "# to calculate rmsd per residue using Bio.PDB\n",
    "def rmsd_pdb_perResidue(moving_pdb, ref_pdb, moving_fasta, ref_fasta):\n",
    "    # load PDB structures\n",
    "    parser = PDBParser()\n",
    "    struct_moving = parser.get_structure(os.path.basename(moving_pdb), moving_pdb)\n",
    "    struct_ref = parser.get_structure(os.path.basename(ref_pdb), ref_pdb)\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "    \n",
    "    # get CA atoms from PDBs\n",
    "    moving = [atom for atom in struct_moving[0].get_atoms() if (atom.name == \"CA\") and (atom.full_id[3][1] in indices_moving)]\n",
    "    fixed = [atom for atom in struct_ref[0].get_atoms() if (atom.name == \"CA\") and (atom.full_id[3][1] in indices_ref)]\n",
    "\n",
    "    # check if the two structures have the same number of length\n",
    "    if len(moving) != len(fixed):\n",
    "        print(\"Two structures have different numbers of residues!\")\n",
    "    \n",
    "    # get the fixed coordinates\n",
    "    coords_fixed = []\n",
    "    for i in range(len(fixed)):\n",
    "        coords_fixed.append(fixed[i].get_coord())\n",
    "    # get the moving coordinates\n",
    "    sup = Superimposer()\n",
    "    sup.set_atoms(fixed, moving)\n",
    "    sup.apply(moving)\n",
    "    coords_moving = []\n",
    "    for i in range(len(moving)):\n",
    "        coords_moving.append(moving[i].get_coord())\n",
    "\n",
    "    # calculate rmsd per residue (CA)\n",
    "    rmsd_perResidue = {}\n",
    "    for i in range(len(coords_fixed)):\n",
    "        residue_id = fixed[i].full_id[3][1]\n",
    "        rmsd_perResidue[residue_id] = rmsd_point(coords_fixed[i], coords_moving[i])\n",
    "\n",
    "    return rmsd_perResidue\n",
    "\n",
    "\n",
    "# to calculate rmsd overall using Bio.PDB\n",
    "# return rmsd between two structures\n",
    "# indices consistent with the previous function \"rmsd_pdb_perResidue\"\n",
    "def rmsd_pdb_overall(moving_pdb, ref_pdb, moving_fasta, ref_fasta):\n",
    "    # load PDB structures\n",
    "    parser = PDBParser()\n",
    "    struct_moving = parser.get_structure(os.path.basename(moving_pdb), moving_pdb)\n",
    "    struct_ref = parser.get_structure(os.path.basename(ref_pdb), ref_pdb)\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "    \n",
    "    # get CA atoms from PDBs\n",
    "    moving = [atom for atom in struct_moving[0].get_atoms() if (atom.name == \"CA\") and (atom.full_id[3][1] in indices_moving)]\n",
    "    fixed = [atom for atom in struct_ref[0].get_atoms() if (atom.name == \"CA\") and (atom.full_id[3][1] in indices_ref)]\n",
    "\n",
    "    # check if the two structures have the same number of length\n",
    "    if len(moving) != len(fixed):\n",
    "        print(\"Two structures have different numbers of residues!\")\n",
    "    \n",
    "    # get the fixed coordinates\n",
    "    coords_fixed = []\n",
    "    for i in range(len(fixed)):\n",
    "        coords_fixed.append(fixed[i].get_coord())\n",
    "    # get the moving coordinates\n",
    "    sup = Superimposer()\n",
    "    sup.set_atoms(fixed, moving)\n",
    "    sup.apply(moving)\n",
    "    coords_moving = []\n",
    "    for i in range(len(moving)):\n",
    "        coords_moving.append(moving[i].get_coord())\n",
    "\n",
    "    rmsd = rmsd_list(coords_moving, coords_fixed)\n",
    "\n",
    "    return rmsd\n",
    "\n",
    "\n",
    "\n",
    "#### delta delta G ####\n",
    "    \n",
    "# extract delta G per residue out of sc file\n",
    "def dG_perResidue(sc_path):\n",
    "    dG_perResidue = {}\n",
    "    with open(sc_path, 'r') as f:\n",
    "        for count, line in enumerate(f.readlines()):\n",
    "            if(count != 0):\n",
    "                line = line.strip(\"\\n\").split()\n",
    "                id = int(line[23].split(\"_\")[1])\n",
    "                score = float(line[22])\n",
    "                dG_perResidue[id] = score\n",
    "    return dG_perResidue\n",
    "\n",
    "# to calculate ddG within pairs\n",
    "def ddG_perResidue(moving_sc, ref_sc, moving_fasta, ref_fasta):\n",
    "    #### input csv files !!! ####\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "\n",
    "    # read score file\n",
    "    dG_moving = pd.read_csv(moving_sc)\n",
    "    dG_ref = pd.read_csv(ref_sc)\n",
    "\n",
    "    # initiate dG and ddG dictionary\n",
    "    dG_iso_perResidue = {}\n",
    "    dG_ref_perResidue = {}\n",
    "    ddG_perResidue = {}\n",
    "    \n",
    "    # dG and ddG\n",
    "    for i in range(len(indices_ref)):\n",
    "        idx_moving = indices_moving[i]\n",
    "        score_moving = dG_moving[dG_moving['residue_id']==idx_moving]['dG'].iloc[0]\n",
    "        idx_ref = indices_ref[i]\n",
    "        score_ref = dG_ref[dG_ref['residue_id']==idx_ref]['dG'].iloc[0]\n",
    "        ddG = round(score_moving - score_ref, 3)\n",
    "\n",
    "        # use index for reference as residue id\n",
    "        dG_iso_perResidue[idx_ref] = score_moving\n",
    "        dG_ref_perResidue[idx_ref] = score_ref\n",
    "        ddG_perResidue[idx_ref] = ddG\n",
    "\n",
    "    return ddG_perResidue, dG_iso_perResidue, dG_ref_perResidue\n",
    "    \n",
    "\n",
    "#### extract fa_sol term out of sc files ####\n",
    "def fa_sol_perResidue(sc_path):\n",
    "    fa_sol_perResidue = {}\n",
    "    with open(sc_path, 'r') as f:\n",
    "        for count, line in enumerate(f.readlines()):\n",
    "            if (count != 0):\n",
    "                line = line.strip(\"\\n\").split()\n",
    "                id = int(line[23].split(\"_\")[1])\n",
    "                fa_sol = float(line[5])\n",
    "                fa_sol_perResidue[id] = fa_sol\n",
    "    return fa_sol_perResidue\n",
    "\n",
    "    \n",
    "# get fa_sol within pairs\n",
    "def get_pair_fa_sol(moving_sc, ref_sc, moving_fasta, ref_fasta):\n",
    "    #### input csv files !!! ####\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "\n",
    "    # read score file\n",
    "    fa_sol_moving = pd.read_csv(moving_sc)\n",
    "    fa_sol_ref = pd.read_csv(ref_sc)\n",
    "\n",
    "    # initiate fa_sol dictionary\n",
    "    fa_sol_iso_perResidue = {}\n",
    "    fa_sol_ref_perResidue = {}\n",
    "\n",
    "    # fa_sol\n",
    "    for i in range(len(indices_ref)):\n",
    "        idx_moving = indices_moving[i]\n",
    "        score_moving = fa_sol_moving[fa_sol_moving['residue_id']==idx_moving]['fa_sol'].iloc[0]\n",
    "        idx_ref = indices_ref[i]\n",
    "        score_ref = fa_sol_ref[fa_sol_ref['residue_id']==idx_ref]['fa_sol'].iloc[0]\n",
    "\n",
    "        # use index for reference as residue id\n",
    "        fa_sol_iso_perResidue[idx_ref] = score_moving\n",
    "        fa_sol_ref_perResidue[idx_ref] = score_ref\n",
    "\n",
    "    return fa_sol_iso_perResidue, fa_sol_ref_perResidue\n",
    "\n",
    "    \n",
    "# get tags with the lowest relax score\n",
    "# here it generates a tag file (basically a txt file)\n",
    "# that contains 3 ids with lowest score by default\n",
    "\n",
    "def get_lowestTag(sc_file, tag_file, num = 3):\n",
    "    if sc_file==tag_file:\n",
    "        print(\"score file and tag file should be different!\")\n",
    "        return 1\n",
    "    # sc file is the one generated by Rosetta Relax\n",
    "    # tag file is a path, not a folder, need to specify the file name\n",
    "    scores_and_ids = pd.DataFrame(columns = ['score', 'id'])\n",
    "    with open(sc_file, \"r\") as f:\n",
    "        for count, line in enumerate(f.readlines()):\n",
    "            if (count != 0) and (count != 1): # excluded the first two lines\n",
    "                line = line.strip(\"\\n\")\n",
    "                line = line.split()\n",
    "                scores_and_ids.loc[len(scores_and_ids)] = [float(line[1]), line[23]]\n",
    "    scores_and_ids = scores_and_ids.sort_values(by = 'score', ascending = True)\n",
    "    lowest_ids = scores_and_ids['id'].head(num)\n",
    "    with open(tag_file, 'w') as f:\n",
    "        for id in lowest_ids:\n",
    "            f.write(f\"{id}\\n\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "# get the lowest relax score\n",
    "def get_lowestScore(sc_file):\n",
    "    # very similar to the function get_lowestTag\n",
    "    # sc file is the one generated by Rosetta Relax\n",
    "    scores_and_ids = pd.DataFrame(columns = ['score', 'id'])\n",
    "    with open(sc_file, 'r') as f:\n",
    "        for count, line in enumerate(f.readlines()):\n",
    "            if (count != 0) and (count != 1): # excluded the first two lines\n",
    "                line = line.strip(\"\\n\")\n",
    "                line = line.split()\n",
    "                scores_and_ids.loc[len(scores_and_ids)] = [float(line[1]), line[23]]\n",
    "    scores_and_ids = scores_and_ids.sort_values(by = 'score', ascending = True)\n",
    "    lowest_score = float(scores_and_ids['score'].head(1))\n",
    "    lowest_tag = scores_and_ids['id'].head(1).iloc[0]\n",
    "    return lowest_score, lowest_tag\n",
    "    \n",
    "\n",
    "#### plddt score (with pair) ####\n",
    "    \n",
    "# get plddt score from \"ranking_debug.json\"\n",
    "def get_plddt(af_ranking_file):\n",
    "    af_ranking = json.load(open(af_ranking_file))\n",
    "    ave_plddt = format(sum(af_ranking['plddts'].values()) / len(af_ranking['plddts'].values()), '.3f')\n",
    "    return float(ave_plddt)\n",
    "\n",
    "\n",
    "# get lowest plddt score from \"ranking_debug.json\"\n",
    "def get_plddt_highest(af_ranking_file):\n",
    "    af_ranking = json.load(open(af_ranking_file))\n",
    "    plddt = max(af_ranking['plddts'].values())\n",
    "    return plddt\n",
    "\n",
    "# get plddt per residue\n",
    "# based on the original residue id within the corresponding pdb file\n",
    "def get_plddt_perResidue(result_model_pkl):\n",
    "    plddt_dic = {} # create a dictionary to store the plddt score\n",
    "    plddt = pd.read_pickle(result_model_pkl)\n",
    "    plddt = plddt['plddt']\n",
    "    for i in range(len(plddt)):\n",
    "        plddt_dic[i+1] = plddt[i]\n",
    "    return plddt_dic\n",
    "\n",
    "\n",
    "# get model name for highest ranking\n",
    "def get_ranked_0_model(af_ranking_file):\n",
    "    af_ranking = json.load(open(af_ranking_file))\n",
    "    model_name = af_ranking['order'][0]\n",
    "    return model_name\n",
    "\n",
    "\n",
    "# get plddt per residue, index based on pairwise sequence alignment\n",
    "def plddt_pair_perResidue(moving_pkl, ref_pkl, moving_fasta, ref_fasta):\n",
    "    ### input pkl files\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "\n",
    "    # read pkl files\n",
    "    moving_plddt = get_plddt_perResidue(moving_pkl)\n",
    "    ref_plddt = get_plddt_perResidue(ref_pkl)\n",
    "\n",
    "    # initiate plddt dictionary\n",
    "    moving_plddt_perResidue = {}\n",
    "    ref_plddt_perResidue = {}\n",
    "    \n",
    "    # change the index based on sequence alignment\n",
    "    for i in range(len(indices_ref)):\n",
    "        idx_moving = indices_moving[i]\n",
    "        idx_ref = indices_ref[i]\n",
    "        plddt_score_moving = moving_plddt[idx_moving]\n",
    "        plddt_score_ref = ref_plddt[idx_ref] # get plddt score from dictionary\n",
    "        moving_plddt_perResidue[idx_ref] = plddt_score_moving # use reference index for consistency\n",
    "        ref_plddt_perResidue[idx_ref] = plddt_score_ref\n",
    "\n",
    "    return moving_plddt_perResidue, ref_plddt_perResidue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#### RSA from DSSP ####\n",
    "\n",
    "\n",
    "# get relative accessible surface\n",
    "def get_dssp(pdb_file):\n",
    "    # need to import #\n",
    "    # from Bio.PDB.DSSP import DSSP\n",
    "    # from Bio.PDB import PDBParser\n",
    "    p = PDBParser()\n",
    "    structure = p.get_structure(str(pdb_file), pdb_file)\n",
    "    model = structure[0]\n",
    "\n",
    "    # count residues\n",
    "    #n = 0\n",
    "    #for res in model.get_residues():\n",
    "        #n += 1\n",
    "    \n",
    "    dssp = DSSP(model, pdb_file, dssp = 'mkdssp')\n",
    "    #if len(dssp.keys()) != n:\n",
    "        #print(os.path.basename(pdb_file) + \" dssp length different from pdb!\")\n",
    "        #return 1\n",
    "    return dssp\n",
    "\n",
    "\n",
    "# change from residue index to dssp index\n",
    "def fix_index(pdb_file, chain_id, residue_id):\n",
    "    # from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
    "\n",
    "    # define chain\n",
    "    #chain_id = os.path.basename(pdb_file).split('.')[0].split('_')[1]\n",
    "\n",
    "    # get DSSP index using residue index\n",
    "    dssp_tuple = dssp_dict_from_pdb_file(pdb_file)\n",
    "    dssp_dict = dssp_tuple[0]\n",
    "    key = (chain_id, (' ', residue_id, ' '))\n",
    "    if key in dssp_dict.keys():\n",
    "        dssp_id = dssp_dict[chain_id, (' ', residue_id, ' ')][5]\n",
    "        # reference: https://github.com/biopython/biopython/blob/master/Bio/PDB/DSSP.py\n",
    "        return dssp_id\n",
    "    return None\n",
    "\n",
    "    \n",
    "# get single RSA value given the residue ID\n",
    "def get_single_rsa(pdb_file, chain_id, residue_id):\n",
    "    dssp_data = get_dssp(pdb_file)\n",
    "    dssp_id = fix_index(pdb_file, chain_id, residue_id)\n",
    "\n",
    "    if dssp_id == None:\n",
    "        return None\n",
    "\n",
    "    for key in dssp_data.keys():\n",
    "        if dssp_data[key][0] == dssp_id:\n",
    "            # dssp_data[key][0] is DSSP index\n",
    "            # according to https://biopython.org/docs/1.76/api/Bio.PDB.DSSP.html\n",
    "            return dssp_data[key][3]\n",
    "    return None\n",
    "\n",
    "    \n",
    "# get RSA per residue, index based on pairwise sequence alignment\n",
    "def get_pair_rsa(moving_pdb, ref_pdb, moving_fasta, ref_fasta, moving_chain_id, ref_chain_id):\n",
    "\n",
    "    # need to import #\n",
    "    # from Bio import SeqIO, pairwise2\n",
    "    # from Bio.PDB.DSSP import DSSP\n",
    "    # from Bio.PDB import PDBParser\n",
    "\n",
    "    # extract common residues\n",
    "    indices_moving, indices_ref = pairwise_SA(moving_fasta, ref_fasta)\n",
    "\n",
    "    # get dssp\n",
    "    #moving_dssp = get_dssp(moving_pdb)\n",
    "    #ref_dssp = get_dssp(ref_pdb)\n",
    "\n",
    "    # initiate rsa dictionary\n",
    "    moving_rsa_perResidue = {}\n",
    "    ref_rsa_perResidue = {}\n",
    "\n",
    "    # index based on reference sequence\n",
    "    for i in range(len(indices_ref)):\n",
    "        idx_moving = indices_moving[i]\n",
    "        idx_ref = indices_ref[i]\n",
    "        \n",
    "        # get RSA\n",
    "        rsa_moving = get_single_rsa(moving_pdb, moving_chain_id, idx_moving)\n",
    "        if rsa_moving != None:\n",
    "            moving_rsa_perResidue[idx_ref] = rsa_moving # save in dictionary, use reference index for consistency\n",
    "            \n",
    "        rsa_ref = get_single_rsa(ref_pdb, ref_chain_id, idx_ref)\n",
    "        if rsa_ref != None:\n",
    "            ref_rsa_perResidue[idx_ref] = rsa_ref\n",
    "\n",
    "    return moving_rsa_perResidue, ref_rsa_perResidue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da06ec-b36d-469b-bc65-066131c370b0",
   "metadata": {},
   "source": [
    "# Define pair IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1177b92-ae05-4f27-9e85-0c0c052c0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pair IDs\n",
    "pair_ids = {'0': 'good',\n",
    "            '1': 'good',\n",
    "            '2': 'good',\n",
    "            '3': 'good',\n",
    "            '4': 'good',\n",
    "            '5': 'good',\n",
    "            '6': 'good',\n",
    "            '7': 'good',\n",
    "            '8': 'good',\n",
    "            '9': 'good',\n",
    "            '10': 'bad',\n",
    "            '11': 'bad',\n",
    "            '13': 'bad'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20815e4f-925e-4843-97f3-ef54c7e51bd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f8eda0-beda-4b8e-a588-b2a7396343c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_11_iso\n",
      "good_1_iso\n",
      "good_9_iso\n",
      "good_4_ref\n",
      "good_4_iso\n",
      "good_6_ref\n",
      "bad_10_iso\n",
      "good_5_iso\n",
      "good_0_ref\n",
      "good_0_iso\n",
      "good_6_iso\n",
      "bad_11_ref\n",
      "good_7_ref\n",
      "bad_13_ref\n",
      "good_2_iso\n",
      "good_9_ref\n",
      "good_3_ref\n",
      "good_5_ref\n",
      "good_8_iso\n",
      "good_3_iso\n",
      "good_7_iso\n",
      "bad_10_ref\n",
      "good_2_ref\n",
      "good_1_ref\n",
      "good_8_ref\n",
      "bad_13_iso\n"
     ]
    }
   ],
   "source": [
    "# extract lowest.tag\n",
    "\n",
    "af_dir = \"alphafold_res\"\n",
    "tag_filename = \"lowest.tag\" # the text file to store the relax structure with lowest\n",
    "\n",
    "for folder in os.listdir(af_dir):\n",
    "    if not folder.startswith('.'):\n",
    "        print(folder)\n",
    "        relax_sc_path = os.path.join(af_dir, folder, \"relax.sc\")\n",
    "        tag_path = os.path.join(af_dir, folder, tag_filename)\n",
    "        get_lowestTag(relax_sc_path, tag_path, num = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca192e98-ff93-4a7d-9688-00228143ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract delta G from sc files\n",
    "for key, value in pair_ids.items():\n",
    "    # get ids\n",
    "    iso_id = value + \"_\" + key + \"_iso\"\n",
    "    ref_id = value + \"_\" + key + \"_ref\"\n",
    "\n",
    "    # sc path\n",
    "    iso_tag_path = os.path.join(\"alphafold_res\", iso_id, \"lowest.tag\")\n",
    "    with open(iso_tag_path, 'r') as f:\n",
    "        iso_relax_name = f.readline().strip()\n",
    "        iso_sc_name = iso_relax_name + \"_perRes.sc\"\n",
    "    iso_sc_path = os.path.join(\"alphafold_res\", iso_id, iso_sc_name)\n",
    "\n",
    "    ref_tag_path = os.path.join(\"alphafold_res\", ref_id, \"lowest.tag\")\n",
    "    with open(ref_tag_path, 'r') as f:\n",
    "        ref_relax_name = f.readline().strip()\n",
    "        ref_sc_name = ref_relax_name + \"_perRes.sc\"\n",
    "    ref_sc_path = os.path.join(\"alphafold_res\", ref_id, ref_sc_name)\n",
    "\n",
    "    # read sc files\n",
    "    sc_iso = dG_perResidue(iso_sc_path)\n",
    "    sc_iso = pd.DataFrame.from_dict(sc_iso, orient = 'index', columns = ['dG'])\n",
    "    sc_iso = sc_iso.reset_index().rename(columns = {'index': 'residue_id'})\n",
    "\n",
    "    sc_ref = dG_perResidue(ref_sc_path)\n",
    "    sc_ref = pd.DataFrame.from_dict(sc_ref, orient = 'index', columns = ['dG'])\n",
    "    sc_ref = sc_ref.reset_index().rename(columns = {'index': 'residue_id'})\n",
    "\n",
    "    # save csv files\n",
    "    iso_csv_name = iso_id + \"_relax.csv\"\n",
    "    iso_csv_path = os.path.join(\"dG_perResidue\", iso_csv_name)\n",
    "    sc_iso.to_csv(iso_csv_path, index = False)\n",
    "\n",
    "    ref_csv_name = ref_id + \"_relax.csv\"\n",
    "    ref_csv_path = os.path.join(\"dG_perResidue\", ref_csv_name)\n",
    "    sc_ref.to_csv(ref_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09c3ea-ba45-4b44-8e9f-46efd71cbbea",
   "metadata": {},
   "source": [
    "# Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "451980d4-6833-4328-b381-fee5147bfa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# loop through pairs\n",
    "\n",
    "tag_filename = \"lowest.tag\"\n",
    "\n",
    "for key, value in pair_ids.items():\n",
    "    # get ids\n",
    "    iso_id = value + \"_\" + key + \"_iso\"\n",
    "    ref_id = value + \"_\" + key + \"_ref\"\n",
    "\n",
    "    # fasta files directory\n",
    "    iso_fasta_name = iso_id + \".fasta\"\n",
    "    iso_fasta_path = os.path.join(\"aa_seq\", iso_fasta_name)\n",
    "    ref_fasta_name = ref_id + \".fasta\"\n",
    "    ref_fasta_path = os.path.join(\"aa_seq\", ref_fasta_name)\n",
    "\n",
    "    # pdb files directory (AF + Relax)\n",
    "    iso_tag_path = os.path.join(\"alphafold_res\", iso_id, tag_filename)\n",
    "    with open(iso_tag_path, 'r') as f:\n",
    "        iso_relax_name = f.readline().strip()\n",
    "        iso_pdb_name = iso_relax_name + \".pdb\"\n",
    "    iso_pdb_path = os.path.join(\"alphafold_res\", iso_id, iso_pdb_name)\n",
    "\n",
    "    ref_tag_path = os.path.join(\"alphafold_res\", ref_id, tag_filename)\n",
    "    with open(ref_tag_path, 'r') as f:\n",
    "        ref_relax_name = f.readline().strip()\n",
    "        ref_pdb_name = ref_relax_name + \".pdb\"\n",
    "    ref_pdb_path = os.path.join(\"alphafold_res\", ref_id, ref_pdb_name)\n",
    "\n",
    "    # csv files that store delta G score information\n",
    "    dG_dir = \"dG_perResidue\"\n",
    "\n",
    "    iso_sc_name = iso_id + \"_relax.csv\"\n",
    "    iso_sc_path = os.path.join(\"dG_perResidue\", iso_sc_name)\n",
    "\n",
    "    ref_sc_name = ref_id + \"_relax.csv\"\n",
    "    ref_sc_path = os.path.join(\"dG_perResidue\", ref_sc_name)\n",
    "    \n",
    "    #### indices for isoform ####\n",
    "    indices_iso, indices_ref = pairwise_SA(iso_fasta_path, ref_fasta_path)\n",
    "    indices_dict = dict(zip(indices_ref, indices_iso)) # use reference indices as keys\n",
    "    # initialize the dataframe\n",
    "    pair_df = pd.DataFrame({\n",
    "        'Residue': indices_dict.keys(),\n",
    "        'Residue_isoform': indices_dict.values()\n",
    "    })\n",
    "\n",
    "    #### add residue types ####\n",
    "    residues = residue_type(iso_fasta_path, ref_fasta_path)\n",
    "    # combine into dataframe\n",
    "    pair_df['ResidueType'] = pair_df['Residue'].map(residues)\n",
    "\n",
    "\n",
    "    #### RMSD ####\n",
    "    rmsd = rmsd_pdb_perResidue(iso_pdb_path, ref_pdb_path, iso_fasta_path, ref_fasta_path)\n",
    "    # combine into dataframe\n",
    "    pair_df['RMSD'] = pair_df['Residue'].map(rmsd)\n",
    "\n",
    "\n",
    "    #### dG and ddG ####\n",
    "    ddG, dG_iso, dG_ref = ddG_perResidue(iso_sc_path, ref_sc_path, iso_fasta_path, ref_fasta_path) #ddG=iso-ref\n",
    "    # combine into dataframe\n",
    "    pair_df['ddG'] = pair_df['Residue'].map(ddG)\n",
    "    pair_df['dG_iso'] = pair_df['Residue'].map(dG_iso)\n",
    "    pair_df['dG_ref'] = pair_df['Residue'].map(dG_ref)\n",
    "\n",
    "\n",
    "    #### plddt per residue ####\n",
    "    iso_model_name = get_ranked_0_model(os.path.join(\"alphafold_res\", iso_id, \"ranking_debug.json\"))\n",
    "    iso_pkl_name = \"result_\" + iso_model_name + \".pkl\"\n",
    "    iso_pkl_path = os.path.join(\"alphafold_res\", iso_id, iso_pkl_name)\n",
    "\n",
    "    ref_model_name = get_ranked_0_model(os.path.join(\"alphafold_res\", ref_id, \"ranking_debug.json\"))\n",
    "    ref_pkl_name = \"result_\" + ref_model_name + \".pkl\"\n",
    "    ref_pkl_path = os.path.join(\"alphafold_res\", ref_id, ref_pkl_name)\n",
    "\n",
    "    iso_plddt, ref_plddt = plddt_pair_perResidue(iso_pkl_path, ref_pkl_path, iso_fasta_path, ref_fasta_path)\n",
    "    # combine into dataframe\n",
    "    pair_df['plddt_iso'] = pair_df['Residue'].map(iso_plddt)\n",
    "    pair_df['plddt_ref'] = pair_df['Residue'].map(ref_plddt)\n",
    "\n",
    "\n",
    "    #### RSA per residue ####\n",
    "    rsa_iso, rsa_ref = get_pair_rsa(iso_pdb_path, ref_pdb_path, iso_fasta_path, ref_fasta_path, \"A\", \"A\")\n",
    "    # combine into dataframe\n",
    "    pair_df['rsa_iso'] = pair_df['Residue'].map(rsa_iso)\n",
    "    pair_df['rsa_ref'] = pair_df['Residue'].map(rsa_ref)\n",
    "\n",
    "    # print process\n",
    "    print(key)\n",
    "\n",
    "    # save to csv\n",
    "    csv_name = \"pair_\" + value + \"_\" + key + \".csv\"\n",
    "    csv_path = os.path.join(\"pairs_csv\", csv_name)\n",
    "    pair_df.to_csv(csv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542a7b53-d4f1-48c7-8913-f26eaaa124fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### metrics for pairs ####\n",
    "\n",
    "# define hydrophilic & hydrophobic\n",
    "hydrophobic = ['W', 'Y', 'F', 'I', 'L', 'M', 'V', 'A', 'C']\n",
    "neutral = ['T', 'H', 'G', 'S', 'Q']\n",
    "hydrophilic = ['R', 'E', 'N', 'K', 'P', 'D']\n",
    "\n",
    "# save everything in a list\n",
    "rows = []\n",
    "\n",
    "\n",
    "# loop through pairs\n",
    "for key, value in pair_ids.items():\n",
    "    # pair id\n",
    "    pair_id = key + \"_\" + value\n",
    "\n",
    "    # get ids\n",
    "    iso_id = value + \"_\" + key + \"_iso\"\n",
    "    ref_id = value + \"_\" + key + \"_ref\"\n",
    "\n",
    "    # define relax sc path\n",
    "    # this is the overall delta G for every single structure generated by RosettaRelax\n",
    "    sc_relax_iso = os.path.join(\"alphafold_res\", iso_id, \"relax.sc\")\n",
    "    sc_relax_ref = os.path.join(\"alphafold_res\", ref_id, \"relax.sc\")\n",
    "\n",
    "    # define the csv path for current pair\n",
    "    csv_name = \"pair_\" + value + \"_\" + key + \".csv\"\n",
    "    csv_path = os.path.join(\"pairs_csv\", csv_name)\n",
    "    # read csv files\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # overall delta G values for both structures\n",
    "    # get_lowestScore(sc_path)\n",
    "    dG_iso, tag_iso = get_lowestScore(sc_relax_iso)\n",
    "    dG_ref, tag_ref = get_lowestScore(sc_relax_ref)\n",
    "    # 10% quantile for the lowest per-residue delta G values\n",
    "    dG_10q_low_iso = df['dG_iso'].quantile(0.1)\n",
    "    dG_10q_high_iso = df['dG_iso'].quantile(0.9)\n",
    "    dG_10q_low_ref = df['dG_ref'].quantile(0.1)\n",
    "    dG_10q_high_ref = df['dG_ref'].quantile(0.9)\n",
    "\n",
    "    # overall plddt scores for both structures\n",
    "    # define \"ranking_debug.json\" path\n",
    "    json_iso = os.path.join(\"alphafold_res\", iso_id, \"ranking_debug.json\")\n",
    "    json_ref = os.path.join(\"alphafold_res\", ref_id, \"ranking_debug.json\")\n",
    "    # highest overall plddt score\n",
    "    plddt_iso = get_plddt_highest(json_iso)\n",
    "    plddt_ref = get_plddt_highest(json_ref)\n",
    "    # 10% quantile for the lowest per-residue plddt scores\n",
    "    plddt_10q_low_iso = df['plddt_iso'].quantile(0.1)\n",
    "    plddt_10q_high_iso = df['plddt_iso'].quantile(0.9)\n",
    "    plddt_10q_low_ref = df['plddt_ref'].quantile(0.1)\n",
    "    plddt_10q_high_ref = df['plddt_ref'].quantile(0.9)\n",
    "\n",
    "    # median RSA for both structures\n",
    "    rsa_iso = df['rsa_iso'].dropna().median()\n",
    "    rsa_ref = df['rsa_ref'].dropna().median()\n",
    "    # median RSA of hydrophilic residues for both structures\n",
    "    rsa_hydrophilic_iso = df[df['ResidueType'].isin(hydrophilic)]['rsa_iso'].dropna().median()\n",
    "    rsa_hydrophilic_ref = df[df['ResidueType'].isin(hydrophilic)]['rsa_ref'].dropna().median()\n",
    "    # median RSA of hydrophobic residues for both structures\n",
    "    rsa_hydrophobic_iso = df[df['ResidueType'].isin(hydrophobic)]['rsa_iso'].dropna().median()\n",
    "    rsa_hydrophobic_ref = df[df['ResidueType'].isin(hydrophobic)]['rsa_ref'].dropna().median()\n",
    "    # median RSA of each residue type for both structures\n",
    "    rsa_W_iso = df[df['ResidueType']=='W']['rsa_iso'].dropna().median()\n",
    "    rsa_W_ref = df[df['ResidueType']=='W']['rsa_ref'].dropna().median() # Tryptophan\n",
    "    rsa_Y_iso = df[df['ResidueType']=='Y']['rsa_iso'].dropna().median()\n",
    "    rsa_Y_ref = df[df['ResidueType']=='Y']['rsa_ref'].dropna().median() # Tyrosine\n",
    "    rsa_F_iso = df[df['ResidueType']=='F']['rsa_iso'].dropna().median()\n",
    "    rsa_F_ref = df[df['ResidueType']=='F']['rsa_ref'].dropna().median() # Phenylalanine\n",
    "    rsa_I_iso = df[df['ResidueType']=='I']['rsa_iso'].dropna().median()\n",
    "    rsa_I_ref = df[df['ResidueType']=='I']['rsa_ref'].dropna().median() # Isoleucine\n",
    "    rsa_L_iso = df[df['ResidueType']=='L']['rsa_iso'].dropna().median()\n",
    "    rsa_L_ref = df[df['ResidueType']=='L']['rsa_ref'].dropna().median() # Leucine\n",
    "    rsa_M_iso = df[df['ResidueType']=='M']['rsa_iso'].dropna().median()\n",
    "    rsa_M_ref = df[df['ResidueType']=='M']['rsa_ref'].dropna().median() # Methionine\n",
    "    rsa_V_iso = df[df['ResidueType']=='V']['rsa_iso'].dropna().median()\n",
    "    rsa_V_ref = df[df['ResidueType']=='V']['rsa_ref'].dropna().median() # Valine\n",
    "    rsa_A_iso = df[df['ResidueType']=='A']['rsa_iso'].dropna().median()\n",
    "    rsa_A_ref = df[df['ResidueType']=='A']['rsa_ref'].dropna().median() # Alanine\n",
    "    rsa_C_iso = df[df['ResidueType']=='C']['rsa_iso'].dropna().median()\n",
    "    rsa_C_ref = df[df['ResidueType']=='C']['rsa_ref'].dropna().median() # Cysteine\n",
    "    rsa_T_iso = df[df['ResidueType']=='T']['rsa_iso'].dropna().median()\n",
    "    rsa_T_ref = df[df['ResidueType']=='T']['rsa_ref'].dropna().median() # Threonine\n",
    "    rsa_H_iso = df[df['ResidueType']=='H']['rsa_iso'].dropna().median()\n",
    "    rsa_H_ref = df[df['ResidueType']=='H']['rsa_ref'].dropna().median() # Histidine\n",
    "    rsa_G_iso = df[df['ResidueType']=='G']['rsa_iso'].dropna().median()\n",
    "    rsa_G_ref = df[df['ResidueType']=='G']['rsa_ref'].dropna().median() # Glycine\n",
    "    rsa_S_iso = df[df['ResidueType']=='S']['rsa_iso'].dropna().median()\n",
    "    rsa_S_ref = df[df['ResidueType']=='S']['rsa_ref'].dropna().median() # Serine\n",
    "    rsa_Q_iso = df[df['ResidueType']=='Q']['rsa_iso'].dropna().median()\n",
    "    rsa_Q_ref = df[df['ResidueType']=='Q']['rsa_ref'].dropna().median() # Glutamine\n",
    "    rsa_R_iso = df[df['ResidueType']=='R']['rsa_iso'].dropna().median()\n",
    "    rsa_R_ref = df[df['ResidueType']=='R']['rsa_ref'].dropna().median() # Arginine\n",
    "    rsa_E_iso = df[df['ResidueType']=='E']['rsa_iso'].dropna().median()\n",
    "    rsa_E_ref = df[df['ResidueType']=='E']['rsa_ref'].dropna().median() # Glutamic acid\n",
    "    rsa_N_iso = df[df['ResidueType']=='N']['rsa_iso'].dropna().median()\n",
    "    rsa_N_ref = df[df['ResidueType']=='N']['rsa_ref'].dropna().median() # Asparagine\n",
    "    rsa_K_iso = df[df['ResidueType']=='K']['rsa_iso'].dropna().median()\n",
    "    rsa_K_ref = df[df['ResidueType']=='K']['rsa_ref'].dropna().median() # Lysine\n",
    "    rsa_P_iso = df[df['ResidueType']=='P']['rsa_iso'].dropna().median()\n",
    "    rsa_P_ref = df[df['ResidueType']=='P']['rsa_ref'].dropna().median() # Proline\n",
    "    rsa_D_iso = df[df['ResidueType']=='D']['rsa_iso'].dropna().median()\n",
    "    rsa_D_ref = df[df['ResidueType']=='D']['rsa_ref'].dropna().median() # Aspartic acid\n",
    "\n",
    "    # pdb path for both iso and ref\n",
    "    pdb_iso_name = tag_iso + \".pdb\"\n",
    "    pdb_iso_path = os.path.join(\"alphafold_res\", iso_id, pdb_iso_name)\n",
    "    pdb_ref_name = tag_ref + \".pdb\"\n",
    "    pdb_ref_path = os.path.join(\"alphafold_res\", ref_id, pdb_ref_name)\n",
    "\n",
    "    # fasta files for both iso and ref\n",
    "    iso_fasta_name = iso_id + \".fasta\"\n",
    "    iso_fasta_path = os.path.join(\"aa_seq\", iso_fasta_name)\n",
    "    ref_fasta_name = ref_id + \".fasta\"\n",
    "    ref_fasta_path = os.path.join(\"aa_seq\", ref_fasta_name)\n",
    "\n",
    "    # overall RMSD\n",
    "    # rmsd_pdb_overall(moving_pdb, ref_pdb, moving_fasta, ref_fasta)\n",
    "    rmsd = rmsd_pdb_overall(pdb_iso_path, pdb_ref_path, iso_fasta_path, ref_fasta_path)\n",
    "\n",
    "\n",
    "    #### integrate everything into a dictionary, and append into list\n",
    "    row = {'pair_id': pair_id,\n",
    "           'rmsd': rmsd,\n",
    "           'dG_iso': dG_iso,\n",
    "           'dG_ref': dG_ref,\n",
    "           'dG_10q_low_iso': dG_10q_low_iso,\n",
    "           'dG_10q_high_iso': dG_10q_high_iso,\n",
    "           'dG_10q_low_ref': dG_10q_low_ref,\n",
    "           'dG_10q_high_ref': dG_10q_high_ref,\n",
    "           'plddt_iso': plddt_iso,\n",
    "           'plddt_ref': plddt_ref,\n",
    "           'plddt_10q_low_iso': plddt_10q_low_iso,\n",
    "           'plddt_10q_high_iso': plddt_10q_high_iso,\n",
    "           'plddt_10q_low_ref': plddt_10q_low_ref,\n",
    "           'plddt_10q_high_ref': plddt_10q_high_ref,\n",
    "           'rsa_iso': rsa_iso,\n",
    "           'rsa_ref': rsa_ref,\n",
    "           'rsa_hydrophilic_iso': rsa_hydrophilic_iso,\n",
    "           'rsa_hydrophilic_ref': rsa_hydrophilic_ref,\n",
    "           'rsa_hydrophobic_iso': rsa_hydrophobic_iso,\n",
    "           'rsa_hydrophobic_ref': rsa_hydrophobic_ref,\n",
    "           'rsa_W_iso': rsa_W_iso,\n",
    "           'rsa_W_ref': rsa_W_ref,\n",
    "           'rsa_Y_iso': rsa_Y_iso,\n",
    "           'rsa_Y_ref': rsa_Y_ref,\n",
    "           'rsa_F_iso': rsa_F_iso,\n",
    "           'rsa_F_ref': rsa_F_ref,\n",
    "           'rsa_I_iso': rsa_I_iso,\n",
    "           'rsa_I_ref': rsa_I_ref,\n",
    "           'rsa_L_iso': rsa_L_iso,\n",
    "           'rsa_L_ref': rsa_L_ref,\n",
    "           'rsa_M_iso': rsa_M_iso,\n",
    "           'rsa_M_ref': rsa_M_ref,\n",
    "           'rsa_V_iso': rsa_V_iso,\n",
    "           'rsa_V_ref': rsa_V_ref,\n",
    "           'rsa_A_iso': rsa_A_iso,\n",
    "           'rsa_A_ref': rsa_A_ref,\n",
    "           'rsa_C_iso': rsa_C_iso,\n",
    "           'rsa_C_ref': rsa_C_ref,\n",
    "           'rsa_T_iso': rsa_T_iso,\n",
    "           'rsa_T_ref': rsa_T_ref,\n",
    "           'rsa_H_iso': rsa_H_iso,\n",
    "           'rsa_H_ref': rsa_H_ref,\n",
    "           'rsa_G_iso': rsa_G_iso,\n",
    "           'rsa_G_ref': rsa_G_ref,\n",
    "           'rsa_S_iso': rsa_S_iso,\n",
    "           'rsa_S_ref': rsa_S_ref,\n",
    "           'rsa_Q_iso': rsa_Q_iso,\n",
    "           'rsa_Q_ref': rsa_Q_ref,\n",
    "           'rsa_R_iso': rsa_R_iso,\n",
    "           'rsa_R_ref': rsa_R_ref,\n",
    "           'rsa_E_iso': rsa_E_iso,\n",
    "           'rsa_E_ref': rsa_E_ref,\n",
    "           'rsa_N_iso': rsa_N_iso,\n",
    "           'rsa_N_ref': rsa_N_ref,\n",
    "           'rsa_K_iso': rsa_K_iso,\n",
    "           'rsa_K_ref': rsa_K_ref,\n",
    "           'rsa_P_iso': rsa_P_iso,\n",
    "           'rsa_P_ref': rsa_P_ref,\n",
    "           'rsa_D_iso': rsa_D_iso,\n",
    "           'rsa_D_ref': rsa_D_ref}\n",
    "    \n",
    "    rows.append(row) # save in list\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_df\n",
    "\n",
    "metrics_df.to_csv('metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e973c-6a14-4110-825b-5e1025487859",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def702f7-07d5-444a-8a38-2f3c08fc4c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-50.998, 'ranked_1_0005')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lowestScore(\"alphafold_res/bad_10_iso/relax.sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4b52b2-6e3f-4ebc-a10c-c1cfaa3de5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plddts': {'model_1_pred_0': 95.97107870012965,\n",
       "  'model_2_pred_0': 96.15198577210786,\n",
       "  'model_3_pred_0': 95.92231012818021,\n",
       "  'model_4_pred_0': 95.66374663187953,\n",
       "  'model_5_pred_0': 95.65247339759524},\n",
       " 'order': ['model_2_pred_0',\n",
       "  'model_1_pred_0',\n",
       "  'model_3_pred_0',\n",
       "  'model_4_pred_0',\n",
       "  'model_5_pred_0']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_ranking = json.load(open(\"alphafold_res/bad_10_ref/ranking_debug.json\"))\n",
    "af_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9628857-9b9d-4c78-816d-530ded91a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.15198577210786"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_plddt_highest(\"alphafold_res/bad_10_ref/ranking_debug.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466008e2-748b-461e-9295-6e5174138b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.046520133671045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rmsd_pdb_overall(moving_pdb, ref_pdb, moving_fasta, ref_fasta)\n",
    "rmsd_pdb_overall(\"alphafold_res/good_0_iso/ranked_4_0001.pdb\", \"alphafold_res/good_0_ref/ranked_1_0001.pdb\", \"aa_seq/good_0_iso.fasta\", \"aa_seq/good_0_ref.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ed82eb-278f-4f04-861c-a7fe527d4cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_moving, indices_ref = pairwise_SA(\"aa_seq/good_0_iso.fasta\", \"aa_seq/good_0_ref.fasta\")\n",
    "len(indices_moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d94cef2-8005-4d1e-9b5c-03bb1e74163d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_bad_10_df = pd.read_csv(\"pairs_csv/pair_bad_10.csv\")\n",
    "len(pair_bad_10_df['ResidueType'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-myenv2]",
   "language": "python",
   "name": "conda-env-miniforge3-myenv2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
